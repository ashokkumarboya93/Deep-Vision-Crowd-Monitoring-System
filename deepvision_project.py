# -*- coding: utf-8 -*-
"""DeepVision_Project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1JRTiaMjPb1sKuzod4UnBUTJzAdh_GoAx

# **DeepVision Crowd Monitor: AI for Density Estimation and Overcrowding Detection**
"""

!pip install torch torchvision torchaudio opencv-python-headless numpy scipy scikit-learn matplotlib pillow streamlit twilio pyngrok tqdm

"""# **Task-1**"""

import os
import cv2
import numpy as np
import torch
from torch.utils.data import Dataset, DataLoader
from torchvision import transforms
import matplotlib.pyplot as plt
import scipy.io as sio
from scipy.ndimage import gaussian_filter
from scipy.spatial import KDTree
from PIL import Image

# ==============================
# Dataset Class with Adaptive Gaussian
# ==============================
class ShanghaiTechDataset(Dataset):
    def __init__(self, image_dir, gt_dir, transform=None, use_adaptive=True, fixed_sigma=15):
        self.image_dir = image_dir
        self.gt_dir = gt_dir
        self.transform = transform
        self.use_adaptive = use_adaptive
        self.fixed_sigma = fixed_sigma

        self.image_files = sorted([
            f for f in os.listdir(image_dir)
            if f.lower().endswith(('.jpg', '.png'))
        ])

    def __len__(self):
        return len(self.image_files)

    def __getitem__(self, idx):
        img_name = self.image_files[idx]
        img_path = os.path.join(self.image_dir, img_name)

        img = cv2.imread(img_path)
        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
        h, w = img.shape[:2]

        # load ground-truth points
        gt_name = f"GT_{os.path.splitext(img_name)[0]}.mat"
        gt_path = os.path.join(self.gt_dir, gt_name)
        if not os.path.exists(gt_path):
            raise FileNotFoundError(f"Ground truth file not found: {gt_path}")
        mat = sio.loadmat(gt_path)
        points = mat["image_info"][0, 0][0, 0][0]

        # density map initialization
        density_map = np.zeros((h, w), dtype=np.float32)

        if len(points) > 0:
            # use adaptive sigma
            if self.use_adaptive and len(points) > 3:
                tree = KDTree(points.copy(), leafsize=2048)
                for i, p in enumerate(points):
                    x, y = min(int(p[0]), w - 1), min(int(p[1]), h - 1)
                    sigma = np.mean(tree.query(p, k=4)[0][1:]) * 0.1
                    sigma = max(1, sigma)  # ensure sigma >=1
                    density_map[y, x] = 1
                    density_map = gaussian_filter(density_map, sigma=sigma)
            else:
                # fixed sigma fallback
                for p in points:
                    x, y = min(int(p[0]), w - 1), min(int(p[1]), h - 1)
                    density_map[y, x] = 1
                density_map = gaussian_filter(density_map, sigma=self.fixed_sigma)

        if self.transform:
            img = self.transform(Image.fromarray(img))
            density_map = cv2.resize(density_map, (img.shape[2], img.shape[1]))
        density_map = torch.tensor(density_map, dtype=torch.float32).unsqueeze(0)

        count = density_map.sum().item()
        return img, density_map, count, img_name

# ==============================
# Visualization Function
# ==============================
def visualize_sample(dataset, idx=0, save_path="visualizations/sample.png"):
    os.makedirs(os.path.dirname(save_path), exist_ok=True)
    img, gt, count, name = dataset[idx]

    plt.figure(figsize=(10, 5))
    plt.subplot(1, 2, 1)
    plt.imshow(img.permute(1, 2, 0))
    plt.title(f"Image: {name}")
    plt.subplot(1, 2, 2)
    plt.imshow(gt.squeeze(0), cmap="jet")
    plt.title(f"Density Map\nCount: {count:.1f}")
    plt.colorbar()
    plt.savefig(save_path)
    plt.show()

# ==============================
# Main Program
# ==============================
if __name__ == "__main__":
    image_dir = r"/content/drive/MyDrive/Shang_data/ShanghaiTech/part_A/train_data/images"
    gt_dir = r"/content/drive/MyDrive/Shang_data/ShanghaiTech/part_A/train_data/ground-truth"

    transform = transforms.Compose([
        transforms.ToTensor(),
        transforms.Resize((512, 512))
    ])

    dataset = ShanghaiTechDataset(image_dir=image_dir, gt_dir=gt_dir, transform=transform, use_adaptive=True)
    loader = DataLoader(dataset, batch_size=1, shuffle=False)
    print(f"Total samples: {len(dataset)}")

    # visualize 1st sample
    visualize_sample(dataset, idx=7)

import os
import cv2
import numpy as np
import torch
from torch.utils.data import Dataset, DataLoader
from torchvision import transforms
import matplotlib.pyplot as plt
import scipy.io as sio
from scipy.ndimage import gaussian_filter


class ShanghaiTechDataset(Dataset):
    def __init__(self, image_dir, gt_dir, transform=None, sigma=15):
        self.image_dir = image_dir
        self.gt_dir = gt_dir
        self.transform = transform
        self.sigma = sigma

        self.image_files = sorted([
            f for f in os.listdir(image_dir)
            if f.lower().endswith(('.jpg', '.png'))
        ])

    def __len__(self):
        return len(self.image_files)

    def __getitem__(self, idx):
        img_name = self.image_files[idx]
        img_path = os.path.join(self.image_dir, img_name)


        img = cv2.imread(img_path)
        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)

        gt_name = f"GT_{os.path.splitext(img_name)[0]}.mat"
        gt_path = os.path.join(self.gt_dir, gt_name)

        if not os.path.exists(gt_path):
            raise FileNotFoundError(f"Ground truth file not found: {gt_path}")

        mat = sio.loadmat(gt_path)
        points = mat["image_info"][0, 0][0, 0][0]


        density_map = np.zeros((img.shape[0], img.shape[1]), dtype=np.float32)
        for point in points:
            x, y = min(int(point[0]), img.shape[1]-1), min(int(point[1]), img.shape[0]-1)
            density_map[y, x] = 1

        density_map = gaussian_filter(density_map, sigma=self.sigma)

        if self.transform:
            img = self.transform(img)

        density_map = cv2.resize(density_map, (img.shape[2], img.shape[1]))
        density_map = torch.tensor(density_map, dtype=torch.float32).unsqueeze(0)
        return img, density_map


def visualize_sample(dataset, idx=0, save_path="visualizations/samplep.png"):
    os.makedirs(os.path.dirname(save_path), exist_ok=True)
    img, gt = dataset[idx]

    plt.figure(figsize=(10, 5))
    plt.subplot(1, 2, 1)
    plt.imshow(img.permute(1, 2, 0))
    plt.title("Image")
    plt.subplot(1, 2, 2)
    plt.imshow(gt.squeeze(0), cmap="jet")
    plt.title("Density Map")
    plt.savefig(save_path)
    plt.show()


if __name__ == "__main__":
    image_dir = r"/content/drive/MyDrive/Shang_data/ShanghaiTech/part_A/train_data/images"
    gt_dir = r"/content/drive/MyDrive/Shang_data/ShanghaiTech/part_A/train_data/ground-truth"

    transform = transforms.Compose([
        transforms.ToTensor(),
        transforms.Resize((512, 512))
    ])
    dataset = ShanghaiTechDataset(image_dir=image_dir, gt_dir=gt_dir, transform=transform)
    loader = DataLoader(dataset, batch_size=4, shuffle=True)
    print(f"Total samples: {len(dataset)}")

    visualize_sample(dataset, idx=0)

"""# **Task-2**"""

import os, glob, cv2, torch, torch.nn as nn, scipy.io as sio
import numpy as np, matplotlib.pyplot as plt
from torch.utils.data import Dataset, DataLoader
from torchvision import models, transforms
from scipy.ndimage import gaussian_filter
from scipy.spatial import KDTree
from tqdm import tqdm

# --------------------------
# CONFIG
# --------------------------
IMG_SZ = (512, 512)
BATCH = 4
EPOCHS = 50
LR = 1e-5
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# --------------------------
# Dataset with Adaptive Gaussian
# --------------------------
class CrowdDataset(Dataset):
    def __init__(self, root, train=True, size=IMG_SZ, use_adaptive=True, fixed_sigma=15):
        sub = "train_data" if train else "test_data"
        img_dir = os.path.join(root, sub, "images")
        self.imgs = sorted(glob.glob(os.path.join(img_dir, "*.jpg")) +
                           glob.glob(os.path.join(img_dir, "*.png")))
        self.gts = [p.replace("images", "ground-truth").replace(".jpg", ".mat").replace(".png", ".mat") for p in self.imgs]
        self.size, self.tf = size, transforms.ToTensor()
        self.use_adaptive = use_adaptive
        self.fixed_sigma = fixed_sigma

    def __len__(self): return len(self.imgs)

    def __getitem__(self, i):
        img = cv2.cvtColor(cv2.imread(self.imgs[i]), cv2.COLOR_BGR2RGB)
        oh, ow = img.shape[:2]

        # Load GT points
        mat = sio.loadmat(self.gts[i])
        pts = mat["image_info"][0,0][0,0][0]

        # Init density map
        den = np.zeros((oh, ow), np.float32)
        if len(pts) > 0:
            if self.use_adaptive and len(pts) > 3:
                tree = KDTree(pts.copy(), leafsize=2048)
                for p in pts:
                    x, y = min(int(p[0]), ow-1), min(int(p[1]), oh-1)
                    sigma = np.mean(tree.query(p, k=4)[0][1:]) * 0.1
                    sigma = max(1, sigma)
                    den[y, x] = 1
                    den = gaussian_filter(den, sigma)
            else:
                for p in pts:
                    x, y = min(int(p[0]), ow-1), min(int(p[1]), oh-1)
                    den[y, x] = 1
                den = gaussian_filter(den, self.fixed_sigma)

        # Resize both image & density map
        img = cv2.resize(img, self.size)
        den = cv2.resize(den, self.size)
        den = den * (len(pts) / (den.sum() + 1e-6))  # keep counts consistent

        return self.tf(img), torch.tensor(den).unsqueeze(0)

# --------------------------
# CSRNet Model
# --------------------------
class CSRNet(nn.Module):
    def __init__(self):
        super().__init__()
        vgg = list(models.vgg16(weights=models.VGG16_Weights.IMAGENET1K_V1).features.children())
        self.front = nn.Sequential(*vgg[:23])
        self.back = nn.Sequential(
            nn.Conv2d(512,512,3,1,2,dilation=2), nn.ReLU(),
            nn.Conv2d(512,512,3,1,2,dilation=2), nn.ReLU(),
            nn.Conv2d(512,512,3,1,2,dilation=2), nn.ReLU(),
            nn.Conv2d(512,256,3,1,2,dilation=2), nn.ReLU(),
            nn.Conv2d(256,128,3,1,2,dilation=2), nn.ReLU(),
            nn.Conv2d(128,64,3,1,2,dilation=2), nn.ReLU(),
            nn.Conv2d(64,1,1)
        )
    def forward(self, x): return self.back(self.front(x))

# --------------------------
# Training + Evaluation
# --------------------------
def train_eval(root, epochs=EPOCHS):
    tr = DataLoader(CrowdDataset(root, True), batch_size=BATCH, shuffle=True)
    te = DataLoader(CrowdDataset(root, False), batch_size=BATCH, shuffle=False)

    model = CSRNet().to(DEVICE)
    opt = torch.optim.Adam(model.parameters(), lr=LR)
    lossf = nn.MSELoss()

    train_losses, val_losses = [], []
    for ep in range(epochs):
        model.train(); tl=0
        pbar = tqdm(tr, desc=f"Epoch {ep+1}/{epochs}")
        for x,y in pbar:
            x,y = x.to(DEVICE), y.to(DEVICE)
            pred = model(x)
            loss = lossf(pred, y)
            opt.zero_grad(); loss.backward(); opt.step()
            tl += loss.item()
            pbar.set_postfix(loss=loss.item())
        train_losses.append(tl/len(tr))

        # Validation
        model.eval(); vl=0; preds=[]; gts=[]
        with torch.no_grad():
            for x,y in te:
                x,y = x.to(DEVICE), y.to(DEVICE)
                p = model(x)
                vl += lossf(p,y).item()
                preds += p.cpu().numpy().sum(axis=(1,2,3)).tolist()
                gts += y.cpu().numpy().sum(axis=(1,2,3)).tolist()
        val_losses.append(vl/len(te))
        mae = np.mean(np.abs(np.array(preds)-np.array(gts)))
        rmse = (np.mean((np.array(preds)-np.array(gts))**2))**0.5
        print(f"Epoch {ep+1}: Train={train_losses[-1]:.4f}, Val={val_losses[-1]:.4f}, MAE={mae:.2f}, RMSE={rmse:.2f}")

    # Save model
    torch.save(model.state_dict(), "csrnet.pth")
    print("‚úÖ Model saved as csrnet.pth")

    # Plot losses
    plt.plot(train_losses, label="Train"); plt.plot(val_losses, label="Val")
    plt.legend(); plt.show()

    return model

import os, glob, torch, torch.nn as nn, scipy.io as sio
import numpy as np, cv2, matplotlib.pyplot as plt
from torch.utils.data import Dataset, DataLoader
from torchvision import models, transforms

class CrowdDataset(Dataset):
    def __init__(self, root, train=True, size=(256,256)):
        sub = "train_data" if train else "test_data"
        imgs = sorted(glob.glob(os.path.join(root, sub, "images", ".jpg")) +
                      glob.glob(os.path.join(root, sub, "images", ".png")))
        print(f"[{sub}] Found {len(imgs)} images in {os.path.join(root,sub,'images')}")
        self.imgs = imgs
        self.gts = [p.replace("images","ground-truth").replace(".jpg",".mat").replace(".png",".mat") for p in imgs]
        self.size, self.tf = size, transforms.ToTensor()

    def __len__(self): return len(self.imgs)

    def __getitem__(self, i):
        img=cv2.imread(self.imgs[i]); img=cv2.resize(img,self.size)
        mat=sio.loadmat(self.gts[i])
        pts=mat["image_info"][0,0][0,0][0]
        den=np.zeros(self.size,np.float32)
        for x,y in pts:
            if int(y*self.size[1]/img.shape[0])<self.size[1] and int(x*self.size[0]/img.shape[1])<self.size[0]:
                den[int(y*self.size[1]/img.shape[0]), int(x*self.size[0]/img.shape[1])]=1
        den=cv2.GaussianBlur(den,(15,15),4)
        return self.tf(img), torch.tensor(den).unsqueeze(0)

class CSRNet(nn.Module):
    def __init__(self):
        super().__init__()
        vgg=list(models.vgg16(pretrained=False).features.children())
        self.front=nn.Sequential(*vgg[:23])
        self.back=nn.Sequential(
            nn.Conv2d(512,512,3,1,2,dilation=2),nn.ReLU(),
            nn.Conv2d(512,512,3,1,2,dilation=2),nn.ReLU(),
            nn.Conv2d(512,512,3,1,2,dilation=2),nn.ReLU(),
            nn.Conv2d(512,256,3,1,2,dilation=2),nn.ReLU(),
            nn.Conv2d(256,128,3,1,2,dilation=2),nn.ReLU(),
            nn.Conv2d(128,64,3,1,2,dilation=2),nn.ReLU(),
            nn.Conv2d(64,1,1))
    def forward(self,x): return self.back(self.front(x))


def train_eval(root,epochs=1):
    device="cuda" if torch.cuda.is_available() else "cpu"
    tr=DataLoader(CrowdDataset(root,True),2,shuffle=True)
    te=DataLoader(CrowdDataset(root,False),2,shuffle=False)
    model,lossf,opt=CSRNet().to(device),nn.MSELoss(),torch.optim.Adam(CSRNet().parameters(),1e-5)

    for ep in range(epochs):
        model.train()
        for x,y in tr:
            x,y=x.to(device),y.to(device)
            loss=lossf(model(x),y)
            opt.zero_grad(); loss.backward(); opt.step()
        print(f"Epoch {ep+1} Loss {loss.item():.4f}")


    model.eval()
    x,y=next(iter(te))
    with torch.no_grad(): p=model(x.to(device)).cpu()
    for i in range(min(2,len(x))):
        plt.figure(figsize=(9,3))
        plt.subplot(1,3,1); plt.imshow(x[i].permute(1,2,0)); plt.title("Image"); plt.axis("off")
        plt.subplot(1,3,2); plt.imshow(y[i][0],cmap="jet"); plt.title("GT"); plt.axis("off")
        plt.subplot(1,3,3); plt.imshow(p[i][0],cmap="jet"); plt.title("Pred"); plt.axis("off")
        plt.show()

import os
root = r"/content/drive/MyDrive/Shang_data/ShanghaiTech/part_B"
for subdir, dirs, files in os.walk(root):
    print(subdir, len(files))

"""# **Task-3**"""

import os, glob, cv2, time
import numpy as np, scipy.io as sio, matplotlib.pyplot as plt
import torch, torch.nn as nn, torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
from torchvision import transforms
from tqdm import tqdm
from torch.cuda.amp import autocast, GradScaler

# --------------------------
# CONFIG
# --------------------------
DATA_ROOT = r"/content/drive/MyDrive/Shang_data/ShanghaiTech/part_A"
IMG_SZ = (256,256)
BATCH = 4
EPOCHS = 100
LR = 1e-5
ALERT_TH = 20.0
SIGMA = 4
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# --------------------------
# Dataset
# --------------------------
class CrowdDataset(Dataset):
    def __init__(self, root="", sub="train"):
        self.tf = transforms.ToTensor()
        self.size = IMG_SZ
        img_dir = os.path.join(root, f"{'train_data' if sub=='train' else 'test_data'}","images")
        self.imgs = sorted(glob.glob(os.path.join(img_dir,"*.jpg")) +
                           glob.glob(os.path.join(img_dir,"*.png")))
        if len(self.imgs) == 0:
            raise FileNotFoundError(f"No images found in {img_dir}. Please check path.")
        self.gt_dir = os.path.join(root, f"{'train_data' if sub=='train' else 'test_data'}","ground-truth")

    def __len__(self):
        return len(self.imgs)

    def __getitem__(self,i):
        p=self.imgs[i]
        img=cv2.cvtColor(cv2.imread(p),cv2.COLOR_BGR2RGB)
        oh,ow=img.shape[:2]
        img=cv2.resize(img,self.size)
        base=os.path.splitext(os.path.basename(p))[0]
        cand = os.path.join(self.gt_dir, f"GT_{base}.mat")
        if not os.path.exists(cand):
            cand = os.path.join(self.gt_dir, base+".mat")
        pts = np.zeros((0,2))
        if os.path.exists(cand):
            m=sio.loadmat(cand)
            try:
                pts = np.array(m["image_info"][0,0][0,0][0],dtype=np.float32)
            except Exception as e:
                print(f"[WARN] Could not parse GT file {cand}: {e}")
        if pts.size:
            pts[:,0] = pts[:,0]* (self.size[1]/ow)
            pts[:,1] = pts[:,1]* (self.size[0]/oh)
        den=np.zeros(self.size,dtype=np.float32)
        for pt in pts:
            x=int(round(pt[0])); y=int(round(pt[1]))
            if 0<=x<self.size[1] and 0<=y<self.size[0]:
                den[y,x]+=1
        den = cv2.GaussianBlur(den,(SIGMA*4+1,SIGMA*4+1),SIGMA) if den.sum()>0 else den
        return self.tf(img.astype('float32')/255.0), torch.from_numpy(den[None]).float()

# --------------------------
# Model: Tiny-MCNN
# --------------------------
class TinyMCNN(nn.Module):
    def __init__(self):
        super().__init__()
        self.net = nn.Sequential(
            nn.Conv2d(3,16,3,1,1), nn.ReLU(), nn.MaxPool2d(2),
            nn.Conv2d(16,32,3,1,1), nn.ReLU(), nn.MaxPool2d(2),
            nn.Conv2d(32,64,3,1,1), nn.ReLU(),
            nn.Conv2d(64,1,1)
        )
    def forward(self,x):
        o=self.net(x)
        if o.shape[2:] != x.shape[2:]:
            o = F.interpolate(o, size=(x.shape[2], x.shape[3]), mode='bilinear', align_corners=False)
        return o

# --------------------------
# Training + Evaluation
# --------------------------
def train_eval(root, epochs=EPOCHS):
    torch.manual_seed(42)
    tr = DataLoader(CrowdDataset(root,'train'), batch_size=BATCH, shuffle=True, num_workers=2, pin_memory=True)
    te = DataLoader(CrowdDataset(root,'test'),  batch_size=BATCH, shuffle=False, num_workers=2, pin_memory=True)
    model = TinyMCNN().to(DEVICE)
    opt = torch.optim.Adam(model.parameters(), lr=LR)
    lossf = nn.MSELoss()
    scaler = GradScaler()
    train_losses=[]; val_losses=[]
    for ep in range(epochs):
        model.train(); tl=0.0
        pbar = tqdm(tr, desc=f"Epoch {ep+1}/{epochs}")
        for x,y in pbar:
            x,y=x.to(DEVICE, non_blocking=True),y.to(DEVICE, non_blocking=True)
            opt.zero_grad()
            with autocast():
                p=model(x); l=lossf(p,y)
            scaler.scale(l).backward()
            scaler.step(opt)
            scaler.update()
            tl+=l.item()
            pbar.set_postfix(loss=l.item())
        train_losses.append(tl/len(tr) if len(tr)>0 else 0)
        model.eval(); vl=0.0; preds=[]; gts=[]
        with torch.no_grad():
            for x,y in te:
                x,y=x.to(DEVICE, non_blocking=True),y.to(DEVICE, non_blocking=True)
                p=model(x); vl+=lossf(p,y).item()
                preds += p.cpu().numpy().sum(axis=(1,2,3)).tolist()
                gts += y.cpu().numpy().sum(axis=(1,2,3)).tolist()
        val_losses.append(vl/len(te) if len(te)>0 else 0)
        mae = np.mean(np.abs(np.array(preds)-np.array(gts))) if preds else 0
        rmse = (np.mean((np.array(preds)-np.array(gts))**2))**0.5 if preds else 0
        print(f"Ep{ep+1} TrLoss={train_losses[-1]:.4f} ValLoss={val_losses[-1]:.4f} MAE={mae:.2f} RMSE={rmse:.2f}")
    plt.plot(train_losses,label='train'); plt.plot(val_losses,label='val'); plt.legend(); plt.show()
    # visualize predictions
    it=iter(te); shown=0
    with torch.no_grad():
        while shown<2:
            try: x,y = next(it)
            except StopIteration: break
            p = model(x.to(DEVICE)).cpu()
            for i in range(len(x)):
                img=(x[i].permute(1,2,0).numpy()*255).astype('uint8')
                plt.figure(figsize=(9,3))
                plt.subplot(1,3,1); plt.imshow(img); plt.title('Image'); plt.axis('off')
                plt.subplot(1,3,2); plt.imshow(y[i,0],cmap='jet'); plt.title(f'GT {y[i].sum():.1f}')
                plt.subplot(1,3,3); plt.imshow(p[i,0],cmap='jet'); plt.title(f'Pred {p[i].sum():.1f}')
                plt.show(); shown+=1
                if shown>=2: break
    torch.save(model.state_dict(),"tiny_mcnn.pth")
    print("Model saved as tiny_mcnn.pth")
    return model

# --------------------------
# Realtime Inference
# --------------------------
def realtime(model, threshold=ALERT_TH, source=0):
    cap=cv2.VideoCapture(source)
    if not cap.isOpened(): print("Cannot open source"); return
    print("Press q to quit")
    prev=time.time()
    while True:
        ret,frame=cap.read()
        if not ret: break
        h,w=IMG_SZ
        frm=cv2.cvtColor(frame,cv2.COLOR_BGR2RGB); frm=cv2.resize(frm,(w,h))
        t = transforms.ToTensor()(frm.astype('float32')/255.0).unsqueeze(0).to(DEVICE)
        with torch.no_grad(): den = model(t).cpu().squeeze().numpy()
        cnt = den.sum()
        heat = (den/den.max()*255).astype('uint8') if den.max()>0 else np.zeros_like(den,dtype='uint8')
        heat = cv2.applyColorMap(heat, cv2.COLORMAP_JET)
        overlay = cv2.addWeighted(cv2.resize(frame,(w,h)),0.6,heat,0.4,0)
        cv2.putText(overlay,f"Count:{cnt:.1f}",(10,25),cv2.FONT_HERSHEY_SIMPLEX,0.8,(0,255,0),2)
        if cnt>threshold: cv2.putText(overlay,"ALERT: OVERCROWD",(10,55),cv2.FONT_HERSHEY_SIMPLEX,0.8,(0,0,255),2)
        fps=1.0/(time.time()-prev); prev=time.time()
        cv2.putText(overlay,f"FPS:{fps:.1f}",(10,85),cv2.FONT_HERSHEY_SIMPLEX,0.8,(255,255,0),2)
        cv2.imshow("Crowd",overlay)
        if cv2.waitKey(1)&0xFF==ord('q'): break
    cap.release(); cv2.destroyAllWindows()

# --------------------------
# MAIN
# --------------------------
if __name__=="__main__":
    model = train_eval(DATA_ROOT, epochs=EPOCHS)
    # Uncomment to run realtime inference after training
    # model.load_state_dict(torch.load("tiny_mcnn.pth", map_location=DEVICE))
    # model.eval()
    # realtime(model, threshold=ALERT_TH, source=0)



"""write hear

# **Task-4**
"""

# Commented out IPython magic to ensure Python compatibility.
# %%writefile app_task4.py
# # Milestone 4: Dashboard + Alerts (Modern Blue Theme)
# 
# import os, smtplib, cv2, torch
# import numpy as np
# import streamlit as st
# from torchvision import transforms
# from PIL import Image
# from twilio.rest import Client
# import torch.nn as nn
# import torch.nn.functional as F
# import matplotlib.pyplot as plt
# import io
# 
# # ---------------- CONFIG ----------------
# IMG_SZ = (256,256)
# DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")
# ALERT_THRESHOLD = int(os.getenv("ALERT_THRESHOLD", "150"))
# 
# # ---------------- MODEL ----------------
# class TinyMCNN(nn.Module):
#     def __init__(self):
#         super().__init__()
#         self.net = nn.Sequential(
#             nn.Conv2d(3,16,3,1,1), nn.ReLU(), nn.MaxPool2d(2),
#             nn.Conv2d(16,32,3,1,1), nn.ReLU(), nn.MaxPool2d(2),
#             nn.Conv2d(32,64,3,1,1), nn.ReLU(),
#             nn.Conv2d(64,1,1)
#         )
#     def forward(self,x):
#         o=self.net(x)
#         if o.shape[2:] != x.shape[2:]:
#             o = F.interpolate(o, size=(x.shape[2], x.shape[3]), mode='bilinear', align_corners=False)
#         return o
# 
# # ---------------- LOAD MODEL ----------------
# MODEL_PATH = "/content/tiny_mcnn.pth"
# model = TinyMCNN().to(DEVICE)
# if os.path.exists(MODEL_PATH):
#     model.load_state_dict(torch.load(MODEL_PATH, map_location=DEVICE))
#     model.eval()
# else:
#     st.error("‚ùå Model file not found. Please place tiny_mcnn.pth in app folder.")
# 
# # ---------------- FUNCTIONS ----------------
# def predict_count(img: Image.Image):
#     tf = transforms.ToTensor()
#     img_resized = img.resize(IMG_SZ)
#     t = tf(np.array(img_resized).astype('float32')/255.0).unsqueeze(0).to(DEVICE)
#     with torch.no_grad():
#         den = model(t).cpu().squeeze().numpy()
#     cnt = float(den.sum())
#     return cnt, den
# 
# def send_email(subject, body):
#     try:
#         EMAIL_USER = os.getenv("EMAIL_USER")
#         EMAIL_PASS = os.getenv("EMAIL_PASS")
#         ALERT_EMAIL = os.getenv("ALERT_EMAIL")
#         msg = f"Subject:{subject}\n\n{body}"
#         with smtplib.SMTP_SSL("smtp.gmail.com", 465) as smtp:
#             smtp.login(EMAIL_USER, EMAIL_PASS)
#             smtp.sendmail(EMAIL_USER, ALERT_EMAIL, msg)
#         st.success("üìß Email alert sent!")
#     except Exception as e:
#         st.error(f"Email failed: {e}")
# 
# def send_sms(body):
#     try:
#         TWILIO_SID = os.getenv("TWILIO_SID")
#         TWILIO_TOKEN = os.getenv("TWILIO_TOKEN")
#         TWILIO_FROM = os.getenv("TWILIO_FROM")
#         ALERT_PHONE = os.getenv("ALERT_PHONE")
#         client = Client(TWILIO_SID, TWILIO_TOKEN)
#         client.messages.create(body=body, from_=TWILIO_FROM, to=ALERT_PHONE)
#         st.success("üì± SMS alert sent!")
#     except Exception as e:
#         st.error(f"SMS failed: {e}")
# 
# # ---------------- PAGE CONFIG ----------------
# st.set_page_config(page_title="Crowd Counting Dashboard", layout="wide", page_icon="üë•")
# 
# # ---------------- CUSTOM CSS ----------------
# st.markdown("""
# <style>
# body {
#     background-color: #f4f9ff;
# }
# .stApp {
#     background: linear-gradient(135deg, #e0f0ff, #f8fbff);
# }
# h1, h2, h3 {
#     color: #0d47a1 !important;
#     font-weight: 700;
# }
# [data-testid="stMetricValue"] {
#     color: #1565c0 !important;
#     font-size: 32px;
#     font-weight: bold;
# }
# .stButton>button {
#     background: linear-gradient(90deg, #1565c0, #1e88e5);
#     color: white;
#     border-radius: 12px;
#     padding: 12px 26px;
#     border: none;
#     font-size: 16px;
#     font-weight: 600;
#     transition: 0.3s;
# }
# .stButton>button:hover {
#     background: linear-gradient(90deg, #1e88e5, #1565c0);
#     transform: scale(1.05);
# }
# .alert {
#     padding: 20px;
#     border-radius: 15px;
#     background: #ffebee;
#     color: #c62828;
#     font-weight: bold;
#     border: 2px solid #ef5350;
#     font-size: 18px;
#     text-align: center;
#     margin-top: 15px;
# }
# .card {
#     background: white;
#     padding: 20px;
#     border-radius: 15px;
#     box-shadow: 0 8px 20px rgba(0,0,0,0.1);
#     margin-bottom: 20px;
# }
# </style>
# """, unsafe_allow_html=True)
# 
# # ---------------- SIDEBAR ----------------
# with st.sidebar:
#     st.header("‚öôÔ∏è Settings")
#     ALERT_THRESHOLD = st.slider("Alert Threshold", 10, 500, ALERT_THRESHOLD)
#     st.markdown("---")
#     st.markdown("Upload a crowd image to get **real-time crowd count** and alerts if threshold exceeds.")
# 
# # ---------------- MAIN DASHBOARD ----------------
# st.title("üë• Real-Time Crowd Counting Dashboard")
# st.markdown("Track crowds and get **automatic alerts** when limits are exceeded.")
# 
# uploaded = st.file_uploader("üìÇ Upload a crowd image", type=["jpg","png","jpeg"])
# 
# if uploaded:
#     img = Image.open(uploaded).convert("RGB")
#     st.image(img, caption="üì∏ Uploaded Image", use_container_width=True)
# 
#     # prediction
#     count, density = predict_count(img)
# 
#     # Display metrics in cards
#     col1, col2, col3 = st.columns(3)
#     with col1:
#         st.markdown(f"<div class='card'><h3>Estimated Crowd</h3><h2>{count:.1f}</h2></div>", unsafe_allow_html=True)
#     with col2:
#         status_text = "üö® High Crowd!" if count > ALERT_THRESHOLD else "‚úÖ Normal"
#         st.markdown(f"<div class='card'><h3>Status</h3><h2>{status_text}</h2></div>", unsafe_allow_html=True)
#     with col3:
#         st.markdown(f"<div class='card'><h3>Threshold</h3><h2>{ALERT_THRESHOLD}</h2></div>", unsafe_allow_html=True)
# 
#     # show density heatmap in a card
#     fig, ax = plt.subplots(figsize=(6,4))
#     ax.imshow(density, cmap="Blues")
#     ax.set_title("Predicted Density Map", fontsize=16)
#     ax.axis("off")
#     buf = io.BytesIO()
#     plt.savefig(buf, format="png", bbox_inches='tight')
#     st.image(buf, caption="Density Heatmap", use_column_width=True)
# 
#     # alert box
#     if count > ALERT_THRESHOLD:
#         st.markdown(f"<div class='alert'>üö® Crowd Alert! Count reached {count:.1f}</div>", unsafe_allow_html=True)
#         if st.button("üì¢ Send Alert"):
#             msg = f"Crowd Alert! Count reached {count:.1f}"
#             send_email("Crowd Alert", msg)
#             send_sms(msg)
#

!pip install -q streamlit pyngrok twilio opencv-python-headless matplotlib pillow scipy

import os
os.environ["TWILIO_SID"] = "xxxxxxxxxxxxx"
os.environ["TWILIO_TOKEN"] = "xxxxxxxxxxxxxx"
os.environ["TWILIO_FROM"] = "+xxxx"
os.environ["ALERT_PHONE"] = "+91xxxxxxxxxxx"

os.environ["EMAIL_USER"] = "xxxxxxxxxx@gmail.com"
os.environ["EMAIL_PASS"] = "rpemwfbxxxxxxxuvudxwnzx"
os.environ["ALERT_EMAIL"] = "xxxxxxxxxxxx@gmail.com"

os.environ["NGROK_AUTH_TOKEN"] = "xxxxxxxxxxxxxx"   # optional but recommended

from pyngrok import ngrok
import time

# Kill old ngrok processes to avoid "max tunnel" error
ngrok.kill()

# Authenticate ngrok
ngrok.set_auth_token(os.environ["NGROK_AUTH_TOKEN"])

# Launch Streamlit app in background
get_ipython().system_raw("streamlit run app_task4.py --server.port 8501 --server.address 0.0.0.0 &")

# Wait a few seconds for Streamlit to start
time.sleep(5)

# Open a public ngrok URL
public_url = ngrok.connect(8501)
print("üåç Open this link:", public_url)

# -------------------------------
# ‚úÖ Notes:
# -------------------------------
# 1. Ensure your credentials (Twilio, Email, Ngrok) are correct.
# 2. tiny_mcnn.pth must be in the same folder as app_task4.py.
# 3. If you restart Colab, rerun this cell to restart ngrok + Streamlit.





